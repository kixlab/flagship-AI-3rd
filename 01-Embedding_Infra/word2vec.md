# Word2Vec Implementation (skip-gram)

## Process
0. Convert data into txt file (each line contains one sentence)
1. Create word_list & word_dict
2. Create skip_gram dataset
3. Train model with the dataset
4. Check accuracy of the model

## Testing the Model

### t-SNE (t-Stochastcic Neighbor Embedding)
Used for dimensionality reduction & visualization; Normally used for word2vec.

SNE + Solving Crowding Problem => t-SNE

## References

- [t-SNE](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/)
- [On word embeddings - Part 3: The secret ingredients of word2vec](http://ruder.io/secret-word2vec/index.html#wordembeddingsvsdistributionalsemanticsmodels)